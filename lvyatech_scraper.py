#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import json
import os
import re
import time
import hashlib
import html
from pathlib import Path
from typing import Any, Dict, List, Tuple
from urllib.parse import unquote, urljoin, urlparse

import aiofiles
import aiohttp
import requests
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from markdownify import markdownify as md


class LvyaTechScraper:
    def __init__(self):
        self.base_url = "http://www.lvyatech.com:37788"
        self.api_url = f"{self.base_url}/server/index.php?s=/api/item/info"
        self.web_url = f"{self.base_url}/web/#"
        self.headers = {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'en,zh;q=0.9,zh-CN;q=0.8,zh-HK;q=0.7,zh-TW;q=0.6',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Origin': self.base_url,
            'Pragma': 'no-cache',
            'Referer': f'{self.base_url}/web/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
        }
        self.cookies = {
            'cookie_name': 'value',
            'think_language': 'en',
            'PHPSESSID': '7o9vc6er8cjrdioi2reea1ob37'
        }
        self.item_id = "39"
        self.output_dir = Path("lvyatech_html_docs")
        self.browser = None
        self.context = None
        self.processed_pages = set()
        self.save_format = "mhtml"  # å¯é€‰: "html", "mhtml" æˆ– "markdown"
        self.images_dir = self.output_dir / "images"
        self.downloaded_images = {}  # URL -> local filename mapping
        self.session = None

    def sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•çš„å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        filename = ''.join(char for char in filename if ord(char) >= 32)
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        return filename.strip()

    def get_api_data(self, page_id: str) -> Dict[str, Any]:
        """é€šè¿‡APIè·å–é¡µé¢æ•°æ®"""
        data = {
            'item_id': self.item_id,
            'keyword': '',
            'default_page_id': page_id
        }
        
        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                cookies=self.cookies,
                data=data,
                verify=False
            )
            response.raise_for_status()
            result = response.json()
            
            if result.get('error_code') == 0:
                return result.get('data', {})
            else:
                print(f"Error getting page {page_id}: {result}")
                return {}
                
        except Exception as e:
            print(f"Exception getting API data for page {page_id}: {e}")
            return {}

    async def init_browser(self, headless=True):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=self.headers['User-Agent']
        )
        
        # è®¾ç½®cookies
        cookies = []
        for name, value in self.cookies.items():
            cookies.append({
                'name': name,
                'value': value,
                'domain': 'www.lvyatech.com',
                'path': '/'
            })
        await self.context.add_cookies(cookies)
        
        # åˆå§‹åŒ–aiohttp session
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            cookies=self.cookies
        )

    async def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.session:
            await self.session.close()
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()

    async def get_page_content(self, item_id: str, page_id: str) -> tuple[str, bytes]:
        """è·å–åŠ¨æ€æ¸²æŸ“åçš„é¡µé¢å†…å®¹ï¼ˆHTMLæˆ–MHTMLï¼‰"""
        # æ„å»ºæ­£ç¡®çš„URLæ ¼å¼: http://www.lvyatech.com:37788/web/#/{item_id}/{page_id}
        url = f"{self.web_url}/{item_id}/{page_id}"
        print(f"  Loading page: {url}")
        
        page = await self.context.new_page()
        try:
            # å¯¼èˆªåˆ°é¡µé¢
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            
            # å¤šé‡ç­‰å¾…ç­–ç•¥ç¡®ä¿é¡µé¢å®Œå…¨æ¸²æŸ“
            # 1. ç­‰å¾…ç½‘ç»œç©ºé—²
            await page.wait_for_load_state('networkidle')
            
            # 2. ç­‰å¾…å¯èƒ½çš„å†…å®¹å®¹å™¨
            content_selectors = [
                '.page-content',
                '.content-wrapper', 
                '.main-content',
                '[class*="content"]',
                '.markdown-body',
                '#content',
                'article',
                'main',
                '.container',
                'pre',  # ä»£ç å—
                'table' # è¡¨æ ¼
            ]
            
            # å°è¯•ç­‰å¾…ä»»æ„ä¸€ä¸ªå†…å®¹é€‰æ‹©å™¨
            try:
                await page.wait_for_selector(
                    ', '.join(content_selectors), 
                    state='visible',
                    timeout=10000
                )
            except:
                pass
            
            # 3. ç­‰å¾…ç‰¹å®šæ—¶é—´ç¡®ä¿JavaScriptæ‰§è¡Œå®Œæˆ
            await page.wait_for_timeout(3000)
            
            # 4. ç­‰å¾…æ‰€æœ‰å›¾ç‰‡åŠ è½½
            await page.evaluate("""
                () => Promise.all(
                    Array.from(document.images)
                        .filter(img => !img.complete)
                        .map(img => new Promise(resolve => {
                            img.onload = img.onerror = resolve;
                        }))
                )
            """)
            
            # 5. æ£€æŸ¥æ˜¯å¦æœ‰åŠ¨æ€å†…å®¹åŠ è½½æŒ‡ç¤ºå™¨
            loading_selectors = [
                '.loading',
                '.spinner',
                '[class*="load"]',
                '.skeleton'
            ]
            
            # ç­‰å¾…åŠ è½½æŒ‡ç¤ºå™¨æ¶ˆå¤±
            for selector in loading_selectors:
                try:
                    await page.wait_for_selector(selector, state='hidden', timeout=5000)
                except:
                    pass
            
            # 6. æœ€åå†ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ‰€æœ‰å¼‚æ­¥æ“ä½œå®Œæˆ
            await page.wait_for_timeout(2000)
            
            # 7. æ™ºèƒ½ç­‰å¾…ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å®é™…å†…å®¹
            max_wait_time = 15000  # æœ€å¤šç­‰å¾…15ç§’
            start_time = asyncio.get_event_loop().time()
            
            while (asyncio.get_event_loop().time() - start_time) * 1000 < max_wait_time:
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å®è´¨å†…å®¹
                content_check = await page.evaluate("""
                    () => {
                        // è·å–é¡µé¢æ–‡æœ¬å†…å®¹é•¿åº¦
                        const bodyText = document.body ? document.body.innerText.trim() : '';
                        const hasContent = bodyText.length > 100;
                        
                        // æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç å—æˆ–è¡¨æ ¼ç­‰ç‰¹æ®Šå†…å®¹
                        const hasCodeBlocks = document.querySelectorAll('pre, code').length > 0;
                        const hasTables = document.querySelectorAll('table').length > 0;
                        const hasImages = document.querySelectorAll('img').length > 0;
                        
                        // æ£€æŸ¥æ˜¯å¦è¿˜åœ¨åŠ è½½ä¸­
                        const loadingElements = document.querySelectorAll('.loading, .spinner, [class*="load"]');
                        const isLoading = Array.from(loadingElements).some(el => 
                            el.offsetParent !== null && window.getComputedStyle(el).display !== 'none'
                        );
                        
                        return {
                            hasContent: hasContent || hasCodeBlocks || hasTables || hasImages,
                            contentLength: bodyText.length,
                            isLoading: isLoading
                        };
                    }
                """)
                
                if content_check['hasContent'] and not content_check['isLoading']:
                    print(f"    Content loaded (length: {content_check['contentLength']} chars)")
                    break
                
                await page.wait_for_timeout(500)
            
            # è·å–æ¸²æŸ“åçš„HTMLå†…å®¹
            html_content = await page.content()
            
            # æ ¹æ®æ ¼å¼è¿”å›å†…å®¹
            if self.save_format == "mhtml":
                # ä½¿ç”¨CDPè·å–MHTML
                client = await page.context.new_cdp_session(page)
                mhtml_data = await client.send('Page.captureSnapshot', {'format': 'mhtml'})
                mhtml_content = mhtml_data.get('data', '')
                
                # è¿”å›ç©ºHTMLå’ŒMHTMLæ•°æ®
                return "", mhtml_content.encode('utf-8')
            else:
                # å¤„ç†ç›¸å¯¹é“¾æ¥ï¼Œè½¬æ¢ä¸ºç»å¯¹é“¾æ¥
                soup = BeautifulSoup(html_content, 'lxml')
                
                # è½¬æ¢æ‰€æœ‰ç›¸å¯¹é“¾æ¥
                for tag in soup.find_all(['a', 'link']):
                    if tag.get('href'):
                        href = tag['href']
                        if href.startswith('/') and not href.startswith('//'):
                            tag['href'] = self.base_url + href
                
                for tag in soup.find_all(['img', 'script']):
                    if tag.get('src'):
                        src = tag['src']
                        if src.startswith('/') and not src.startswith('//'):
                            tag['src'] = self.base_url + src
                
                return str(soup), b""
            
        except Exception as e:
            print(f"  Error loading page {url}: {e}")
            error_html = f"<html><body><h1>Error loading page</h1><p>{str(e)}</p></body></html>"
            return error_html, b""
        finally:
            await page.close()
    
    async def download_image(self, img_url: str) -> str:
        """ä¸‹è½½å›¾ç‰‡å¹¶è¿”å›æœ¬åœ°è·¯å¾„"""
        try:
            # è§£ç HTMLå®ä½“ï¼ˆå¦‚ &amp; -> &ï¼‰
            img_url = html.unescape(img_url)
            
            # å¦‚æœå·²ç»ä¸‹è½½è¿‡ï¼Œç›´æ¥è¿”å›
            if img_url in self.downloaded_images:
                return self.downloaded_images[img_url]
            
            # ä¸‹è½½å›¾ç‰‡ï¼ˆå…è®¸é‡å®šå‘ï¼‰
            async with self.session.get(img_url, ssl=False, allow_redirects=True) as response:
                if response.status == 200:
                    content = await response.read()
                    
                    # å°è¯•ä»å¤šä¸ªæ¥æºè·å–æ–‡ä»¶å
                    filename = None
                    
                    # 1. ä»Content-Dispositionå¤´è·å–æ–‡ä»¶å
                    content_disposition = response.headers.get('Content-Disposition', '')
                    if content_disposition:
                        import re
                        match = re.search(r'filename[^;=\n]*=(([\'"]).*?\2|[^;\n]*)', content_disposition)
                        if match:
                            filename = match.group(1).strip('"\'')
                    
                    # 2. ä»æœ€ç»ˆURLï¼ˆé‡å®šå‘åï¼‰è·å–æ–‡ä»¶å
                    if not filename:
                        final_url = str(response.url)
                        # å¦‚æœå‘ç”Ÿäº†é‡å®šå‘ï¼Œæ˜¾ç¤ºä¿¡æ¯
                        if final_url != img_url:
                            print(f"      Redirected to: {final_url}")
                        parsed_final_url = urlparse(final_url)
                        filename = os.path.basename(parsed_final_url.path)
                    
                    # 3. ä»åŸå§‹URLè·å–æ–‡ä»¶åï¼ˆå¦‚æœæ²¡æœ‰é‡å®šå‘ï¼‰
                    if not filename or filename in ['index.php', 'download.php', 'file.php']:
                        parsed_url = urlparse(img_url)
                        filename = os.path.basename(parsed_url.path)
                    
                    # 4. å¦‚æœè¿˜æ˜¯æ²¡æœ‰åˆé€‚çš„æ–‡ä»¶åï¼Œä½¿ç”¨é»˜è®¤åç§°
                    if not filename or filename in ['index.php', 'download.php', 'file.php', '']:
                        # å°è¯•ä»Content-Typeè·å–æ‰©å±•å
                        content_type = response.headers.get('Content-Type', '')
                        ext = '.png'  # é»˜è®¤æ‰©å±•å
                        if 'jpeg' in content_type or 'jpg' in content_type:
                            ext = '.jpg'
                        elif 'png' in content_type:
                            ext = '.png'
                        elif 'gif' in content_type:
                            ext = '.gif'
                        elif 'webp' in content_type:
                            ext = '.webp'
                        elif 'svg' in content_type:
                            ext = '.svg'
                        
                        filename = f"image{ext}"
                    
                    # ç¡®ä¿æ–‡ä»¶åæœ‰æ‰©å±•å
                    if '.' not in filename:
                        # ä»Content-Typeè·å–æ‰©å±•å
                        content_type = response.headers.get('Content-Type', '')
                        if 'jpeg' in content_type or 'jpg' in content_type:
                            filename += '.jpg'
                        elif 'png' in content_type:
                            filename += '.png'
                        elif 'gif' in content_type:
                            filename += '.gif'
                        elif 'webp' in content_type:
                            filename += '.webp'
                        else:
                            filename += '.png'  # é»˜è®¤
                    
                    # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆä½¿ç”¨URLå“ˆå¸Œä½œä¸ºå‰ç¼€ï¼‰
                    url_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]
                    local_filename = f"{url_hash}_{filename}"
                    local_path = self.images_dir / local_filename
                    
                    # ä¿å­˜æ–‡ä»¶
                    async with aiofiles.open(local_path, 'wb') as f:
                        await f.write(content)
                    
                    # è®°å½•æ˜ å°„å…³ç³»
                    self.downloaded_images[img_url] = f"images/{local_filename}"
                    print(f"    Downloaded image: {local_filename}")
                    return f"images/{local_filename}"
                else:
                    print(f"    Failed to download image: {img_url} (Status: {response.status})")
                    return img_url
                    
        except Exception as e:
            print(f"    Error downloading image {img_url}: {e}")
            return img_url
    
    async def convert_to_markdown(self, html_content: str, page_url: str) -> str:
        """å°†HTMLè½¬æ¢ä¸ºMarkdownå¹¶å¤„ç†å›¾ç‰‡"""
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ä¸‹è½½å¹¶æ›¿æ¢æ‰€æœ‰å›¾ç‰‡é“¾æ¥
        for img in soup.find_all('img'):
            if img.get('src'):
                img_url = img['src']
                # è½¬æ¢ä¸ºç»å¯¹URL
                if not img_url.startswith(('http://', 'https://')):
                    img_url = urljoin(page_url, img_url)
                
                print(f"    Processing image: {img_url}")
                
                # ä¸‹è½½å›¾ç‰‡
                local_path = await self.download_image(img_url)
                img['src'] = local_path
        
        # è½¬æ¢ä¸ºMarkdown
        markdown_content = md(str(soup), heading_style="ATX", bullets="-")
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
        
        return markdown_content

    async def save_page_content(self, page_info: Dict[str, Any], content: tuple[str, bytes], file_path: Path, page_url: str = None):
        """ä¿å­˜é¡µé¢å†…å®¹ï¼ˆHTMLã€MHTMLæˆ–Markdownï¼‰"""
        try:
            if self.save_format == "mhtml" and content[1]:
                # ä¿å­˜MHTMLæ–‡ä»¶
                with open(file_path, 'wb') as f:
                    f.write(content[1])
                print(f"  Saved MHTML: {file_path}")
            elif self.save_format == "markdown":
                # è½¬æ¢å¹¶ä¿å­˜ä¸ºMarkdown
                html_content = content[0]
                markdown_content = await self.convert_to_markdown(html_content, page_url)
                
                # æ·»åŠ é¡µé¢å…ƒæ•°æ®åˆ°Markdownå¼€å¤´
                page_title = page_info.get('page_title', 'Untitled')
                metadata = f"""# {page_title}

---
- **é¡µé¢ID**: {page_info.get('page_id', '')}
- **ä½œè€…ID**: {page_info.get('author_uid', '')}
- **åˆ›å»ºæ—¶é—´**: {page_info.get('addtime', '')}
- **åˆ†ç±»ID**: {page_info.get('cat_id', '')}
---

"""
                
                # ä¿å­˜Markdownæ–‡ä»¶
                async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                    await f.write(metadata + markdown_content)
                
                print(f"  Saved Markdown: {file_path}")
            else:
                # ä¿å­˜HTMLæ–‡ä»¶
                html_content = content[0]
                
                # åœ¨HTMLä¸­åµŒå…¥é¡µé¢å…ƒæ•°æ®
                soup = BeautifulSoup(html_content, 'lxml')
                
                # æ·»åŠ å…ƒæ•°æ®åˆ°head
                if not soup.head:
                    soup.html.insert(0, soup.new_tag('head'))
                
                # æ·»åŠ é¡µé¢ä¿¡æ¯ä½œä¸ºmetaæ ‡ç­¾
                meta_info = {
                    'page-id': page_info.get('page_id', ''),
                    'page-title': page_info.get('page_title', ''),
                    'author-uid': page_info.get('author_uid', ''),
                    'add-time': page_info.get('addtime', ''),
                    'cat-id': page_info.get('cat_id', '')
                }
                
                for name, content_value in meta_info.items():
                    meta_tag = soup.new_tag('meta', attrs={'name': name, 'content': str(content_value)})
                    soup.head.append(meta_tag)
                
                # ä¿å­˜HTMLæ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                
                print(f"  Saved HTML: {file_path}")
            
        except Exception as e:
            print(f"  Error saving {file_path}: {e}")

    async def process_page(self, page_info: Dict[str, Any], parent_path: Path, item_id: str = None):
        """å¤„ç†å•ä¸ªé¡µé¢"""
        page_id = page_info.get('page_id')
        
        # é¿å…é‡å¤å¤„ç†
        if page_id in self.processed_pages:
            return
        self.processed_pages.add(page_id)
        
        page_title = page_info.get('page_title', 'Untitled')
        page_title_clean = self.sanitize_filename(page_title)
        
        # ä½¿ç”¨ä¼ å…¥çš„item_idæˆ–é»˜è®¤çš„self.item_id
        current_item_id = item_id or self.item_id
        
        print(f"Processing page: {page_title} (Item: {current_item_id}, Page: {page_id})")
        
        # è·å–é¡µé¢å†…å®¹
        content = await self.get_page_content(current_item_id, page_id)
        
        # æ ¹æ®æ ¼å¼ç¡®å®šæ–‡ä»¶æ‰©å±•å
        if self.save_format == "mhtml":
            file_ext = ".mhtml"
        elif self.save_format == "markdown":
            file_ext = ".md"
        else:
            file_ext = ".html"
            
        file_name = f"{page_id}_{page_title_clean}{file_ext}"
        file_path = parent_path / file_name
        
        # æ„å»ºé¡µé¢URL
        page_url = f"{self.web_url}/{current_item_id}/{page_id}"
        
        await self.save_page_content(page_info, content, file_path, page_url)
        
        # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(1)

    async def process_catalog(self, catalog: Dict[str, Any], parent_path: Path, item_id: str = None):
        """é€’å½’å¤„ç†ç›®å½•å’Œé¡µé¢"""
        # è·å–ç›®å½•åç§°å¹¶æ¸…ç†
        cat_name = catalog.get('cat_name', 'Unnamed')
        cat_name_clean = self.sanitize_filename(cat_name)
        cat_path = parent_path / cat_name_clean
        
        # ä½¿ç”¨catalogä¸­çš„item_idæˆ–ä¼ å…¥çš„item_idæˆ–é»˜è®¤å€¼
        current_item_id = catalog.get('item_id') or item_id or self.item_id
        
        print(f"\nProcessing catalog: {cat_name} (Item: {current_item_id})")
        cat_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç›®å½•ç´¢å¼•æ–‡ä»¶
        index_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{cat_name}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        ul {{ list-style-type: none; padding: 0; }}
        li {{ margin: 10px 0; }}
        a {{ text-decoration: none; color: #0066cc; }}
        a:hover {{ text-decoration: underline; }}
    </style>
</head>
<body>
    <h1>{cat_name}</h1>
    <ul>
"""
        
        # å¤„ç†è¯¥ç›®å½•ä¸‹çš„é¡µé¢
        pages = catalog.get('pages', [])
        for page in pages:
            await self.process_page(page, cat_path, current_item_id)
            # æ·»åŠ åˆ°ç´¢å¼•
            page_title = page.get('page_title', 'Untitled')
            page_id = page.get('page_id')
            if self.save_format == "mhtml":
                file_ext = ".mhtml"
            elif self.save_format == "markdown":
                file_ext = ".md"
            else:
                file_ext = ".html"
            file_name = f"{page_id}_{self.sanitize_filename(page_title)}{file_ext}"
            index_content += f'        <li><a href="{file_name}">{page_title}</a></li>\n'
        
        # é€’å½’å¤„ç†å­ç›®å½•
        sub_catalogs = catalog.get('catalogs', [])
        if sub_catalogs:
            index_content += '        <li><hr></li>\n'
            index_content += '        <li><strong>å­ç›®å½•:</strong></li>\n'
            
        for sub_catalog in sub_catalogs:
            sub_cat_name = sub_catalog.get('cat_name', 'Unnamed')
            sub_cat_clean = self.sanitize_filename(sub_cat_name)
            index_content += f'        <li>ğŸ“ <a href="{sub_cat_clean}/index.html">{sub_cat_name}</a></li>\n'
            await self.process_catalog(sub_catalog, cat_path, current_item_id)
        
        index_content += """    </ul>
</body>
</html>"""
        
        # ä¿å­˜ç›®å½•ç´¢å¼•
        with open(cat_path / "index.html", 'w', encoding='utf-8') as f:
            f.write(index_content)

    async def scrape_site(self, headless=True, save_format="mhtml"):
        """çˆ¬å–æ•´ä¸ªç½‘ç«™"""
        self.save_format = save_format
        print(f"Starting to scrape site with item_id: {self.item_id}")
        print(f"Browser mode: {'Headless' if headless else 'Visible'}")
        print(f"Save format: {self.save_format}")
        
        # åˆå§‹åŒ–æµè§ˆå™¨
        await self.init_browser(headless=headless)
        
        try:
            # è·å–ç½‘ç«™ç»“æ„
            site_data = self.get_api_data("539")  # ä½¿ç”¨é»˜è®¤é¡µé¢ID
            
            if not site_data:
                print("Failed to get site structure")
                return
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_dir.mkdir(parents=True, exist_ok=True)
            
            # å¦‚æœæ˜¯Markdownæ ¼å¼ï¼Œåˆ›å»ºimagesç›®å½•
            if self.save_format == "markdown":
                self.images_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç½‘ç«™å…ƒæ•°æ®
            with open(self.output_dir / 'site_structure.json', 'w', encoding='utf-8') as f:
                json.dump(site_data, f, ensure_ascii=False, indent=2)
            
            menu = site_data.get('menu', {})
            
            # åˆ›å»ºä¸»ç´¢å¼•æ–‡ä»¶
            item_name = site_data.get('item_name', 'Documentation')
            index_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{item_name}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        ul {{ list-style-type: none; padding: 0; }}
        li {{ margin: 10px 0; }}
        a {{ text-decoration: none; color: #0066cc; }}
        a:hover {{ text-decoration: underline; }}
        .section {{ margin-top: 30px; }}
    </style>
</head>
<body>
    <h1>{item_name}</h1>
    
    <div class="section">
        <h2>é¡µé¢åˆ—è¡¨</h2>
        <ul>
"""
            
            # å¤„ç†æ ¹ç›®å½•ä¸‹çš„é¡µé¢
            root_pages = menu.get('pages', [])
            current_item_id = site_data.get('item_id', self.item_id)
            
            for page in root_pages:
                await self.process_page(page, self.output_dir, current_item_id)
                # æ·»åŠ åˆ°ä¸»ç´¢å¼•
                page_title = page.get('page_title', 'Untitled')
                page_id = page.get('page_id')
                if self.save_format == "mhtml":
                    file_ext = ".mhtml"
                elif self.save_format == "markdown":
                    file_ext = ".md"
                else:
                    file_ext = ".html"
                file_name = f"{page_id}_{self.sanitize_filename(page_title)}{file_ext}"
                index_content += f'            <li><a href="{file_name}">{page_title}</a></li>\n'
            
            # å¤„ç†ç›®å½•ç»“æ„
            catalogs = menu.get('catalogs', [])
            if catalogs:
                index_content += """        </ul>
    </div>
    
    <div class="section">
        <h2>ç›®å½•ç»“æ„</h2>
        <ul>
"""
            
            for catalog in catalogs:
                cat_name = catalog.get('cat_name', 'Unnamed')
                cat_clean = self.sanitize_filename(cat_name)
                index_content += f'            <li>ğŸ“ <a href="{cat_clean}/index.html">{cat_name}</a></li>\n'
                await self.process_catalog(catalog, self.output_dir, current_item_id)
            
            index_content += """        </ul>
    </div>
</body>
</html>"""
            
            # ä¿å­˜ä¸»ç´¢å¼•æ–‡ä»¶
            with open(self.output_dir / "index.html", 'w', encoding='utf-8') as f:
                f.write(index_content)
            
            print(f"\nScraping completed!")
            print(f"All content saved to: {self.output_dir}")
            print(f"Total pages processed: {len(self.processed_pages)}")
            
        finally:
            # å…³é—­æµè§ˆå™¨
            await self.close_browser()


async def main():
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    headless = True
    save_format = "mhtml"  # é»˜è®¤ä½¿ç”¨MHTMLæ ¼å¼
    
    if '--show-browser' in sys.argv:
        headless = False
        print("Running with visible browser for debugging...")
    
    if '--html' in sys.argv:
        save_format = "html"
        print("Using HTML format (without embedded resources)")
    
    if '--markdown' in sys.argv or '--md' in sys.argv:
        save_format = "markdown"
        print("Using Markdown format (with local images)")
    
    scraper = LvyaTechScraper()
    await scraper.scrape_site(headless=headless, save_format=save_format)


if __name__ == "__main__":
    asyncio.run(main())