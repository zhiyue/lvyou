#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import json
import os
import re
import time
import hashlib
import html
from pathlib import Path
from typing import Any, Dict, List, Tuple
from urllib.parse import unquote, urljoin, urlparse

import aiofiles
import aiohttp
import requests
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from markdownify import markdownify as md


class LvyaTechScraper:
    def __init__(self):
        self.base_url = "http://www.lvyatech.com:37788"
        self.api_url = f"{self.base_url}/server/index.php?s=/api/item/info"
        self.page_api_url = f"{self.base_url}/server/index.php?s=/api/page/info"
        self.web_url = f"{self.base_url}/web/#"
        self.headers = {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'en,zh;q=0.9,zh-CN;q=0.8,zh-HK;q=0.7,zh-TW;q=0.6',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Origin': self.base_url,
            'Pragma': 'no-cache',
            'Referer': f'{self.base_url}/web/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
        }
        self.cookies = {
            'cookie_name': 'value',
            'think_language': 'en',
            'PHPSESSID': '7o9vc6er8cjrdioi2reea1ob37'
        }
        self.item_id = "39"
        self.output_dir = Path("lvyatech_html_docs")
        self.browser = None
        self.context = None
        self.processed_pages = set()
        self.save_format = "mhtml"  # å¯é€‰: "html", "mhtml" æˆ– "markdown"
        self.images_dir = self.output_dir / "images"
        self.downloaded_images = {}  # URL -> local filename mapping
        self.session = None

    def sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•çš„å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        filename = ''.join(char for char in filename if ord(char) >= 32)
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        return filename.strip()

    def get_api_data(self, page_id: str) -> Dict[str, Any]:
        """é€šè¿‡APIè·å–é¡µé¢æ•°æ®"""
        data = {
            'item_id': self.item_id,
            'keyword': '',
            'default_page_id': page_id
        }
        
        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                cookies=self.cookies,
                data=data,
                verify=False
            )
            response.raise_for_status()
            result = response.json()
            
            if result.get('error_code') == 0:
                return result.get('data', {})
            else:
                print(f"Error getting page {page_id}: {result}")
                return {}
                
        except Exception as e:
            print(f"Exception getting API data for page {page_id}: {e}")
            return {}
    
    def get_page_markdown(self, page_id: str, max_retries: int = 3) -> Dict[str, Any]:
        """é€šè¿‡APIç›´æ¥è·å–é¡µé¢çš„Markdownå†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
        data = {
            'page_id': page_id
        }
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    self.page_api_url,
                    headers=self.headers,
                    cookies=self.cookies,
                    data=data,
                    verify=False,
                    timeout=30  # æ·»åŠ è¶…æ—¶è®¾ç½®
                )
                response.raise_for_status()
                result = response.json()
                
                if result.get('error_code') == 0:
                    return result.get('data', {})
                else:
                    print(f"Error getting markdown for page {page_id}: {result}")
                    return {}
                    
            except requests.exceptions.Timeout:
                print(f"  Timeout getting markdown for page {page_id} (attempt {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
            except Exception as e:
                print(f"  Exception getting markdown for page {page_id}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                    
        return {}

    async def init_browser(self, headless=True):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=self.headers['User-Agent']
        )
        
        # è®¾ç½®cookies
        cookies = []
        for name, value in self.cookies.items():
            cookies.append({
                'name': name,
                'value': value,
                'domain': 'www.lvyatech.com',
                'path': '/'
            })
        await self.context.add_cookies(cookies)
        
        # åˆå§‹åŒ–aiohttp session
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            cookies=self.cookies
        )

    async def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.session:
            await self.session.close()
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()

    async def get_page_content(self, item_id: str, page_id: str) -> tuple[str, bytes]:
        """è·å–åŠ¨æ€æ¸²æŸ“åçš„é¡µé¢å†…å®¹ï¼ˆHTMLæˆ–MHTMLï¼‰"""
        # æ„å»ºæ­£ç¡®çš„URLæ ¼å¼: http://www.lvyatech.com:37788/web/#/{item_id}/{page_id}
        url = f"{self.web_url}/{item_id}/{page_id}"
        print(f"  Loading page: {url}")
        
        page = await self.context.new_page()
        try:
            # å¯¼èˆªåˆ°é¡µé¢
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            
            # å¤šé‡ç­‰å¾…ç­–ç•¥ç¡®ä¿é¡µé¢å®Œå…¨æ¸²æŸ“
            # 1. ç­‰å¾…ç½‘ç»œç©ºé—²
            await page.wait_for_load_state('networkidle')
            
            # 2. ç­‰å¾…å¯èƒ½çš„å†…å®¹å®¹å™¨
            content_selectors = [
                '.page-content',
                '.content-wrapper', 
                '.main-content',
                '[class*="content"]',
                '.markdown-body',
                '#content',
                'article',
                'main',
                '.container',
                'pre',  # ä»£ç å—
                'table' # è¡¨æ ¼
            ]
            
            # å°è¯•ç­‰å¾…ä»»æ„ä¸€ä¸ªå†…å®¹é€‰æ‹©å™¨
            try:
                await page.wait_for_selector(
                    ', '.join(content_selectors), 
                    state='visible',
                    timeout=10000
                )
            except:
                pass
            
            # 3. ç­‰å¾…ç‰¹å®šæ—¶é—´ç¡®ä¿JavaScriptæ‰§è¡Œå®Œæˆ
            await page.wait_for_timeout(3000)
            
            # 4. ç­‰å¾…æ‰€æœ‰å›¾ç‰‡åŠ è½½
            await page.evaluate("""
                () => Promise.all(
                    Array.from(document.images)
                        .filter(img => !img.complete)
                        .map(img => new Promise(resolve => {
                            img.onload = img.onerror = resolve;
                        }))
                )
            """)
            
            # 5. æ£€æŸ¥æ˜¯å¦æœ‰åŠ¨æ€å†…å®¹åŠ è½½æŒ‡ç¤ºå™¨
            loading_selectors = [
                '.loading',
                '.spinner',
                '[class*="load"]',
                '.skeleton'
            ]
            
            # ç­‰å¾…åŠ è½½æŒ‡ç¤ºå™¨æ¶ˆå¤±
            for selector in loading_selectors:
                try:
                    await page.wait_for_selector(selector, state='hidden', timeout=5000)
                except:
                    pass
            
            # 6. æœ€åå†ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ‰€æœ‰å¼‚æ­¥æ“ä½œå®Œæˆ
            await page.wait_for_timeout(2000)
            
            # 7. æ™ºèƒ½ç­‰å¾…ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å®é™…å†…å®¹
            max_wait_time = 15000  # æœ€å¤šç­‰å¾…15ç§’
            start_time = asyncio.get_event_loop().time()
            
            while (asyncio.get_event_loop().time() - start_time) * 1000 < max_wait_time:
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å®è´¨å†…å®¹
                content_check = await page.evaluate("""
                    () => {
                        // è·å–é¡µé¢æ–‡æœ¬å†…å®¹é•¿åº¦
                        const bodyText = document.body ? document.body.innerText.trim() : '';
                        const hasContent = bodyText.length > 100;
                        
                        // æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç å—æˆ–è¡¨æ ¼ç­‰ç‰¹æ®Šå†…å®¹
                        const hasCodeBlocks = document.querySelectorAll('pre, code').length > 0;
                        const hasTables = document.querySelectorAll('table').length > 0;
                        const hasImages = document.querySelectorAll('img').length > 0;
                        
                        // æ£€æŸ¥æ˜¯å¦è¿˜åœ¨åŠ è½½ä¸­
                        const loadingElements = document.querySelectorAll('.loading, .spinner, [class*="load"]');
                        const isLoading = Array.from(loadingElements).some(el => 
                            el.offsetParent !== null && window.getComputedStyle(el).display !== 'none'
                        );
                        
                        return {
                            hasContent: hasContent || hasCodeBlocks || hasTables || hasImages,
                            contentLength: bodyText.length,
                            isLoading: isLoading
                        };
                    }
                """)
                
                if content_check['hasContent'] and not content_check['isLoading']:
                    print(f"    Content loaded (length: {content_check['contentLength']} chars)")
                    break
                
                await page.wait_for_timeout(500)
            
            # è·å–æ¸²æŸ“åçš„HTMLå†…å®¹
            html_content = await page.content()
            
            # æ ¹æ®æ ¼å¼è¿”å›å†…å®¹
            if self.save_format == "mhtml":
                # ä½¿ç”¨CDPè·å–MHTML
                client = await page.context.new_cdp_session(page)
                mhtml_data = await client.send('Page.captureSnapshot', {'format': 'mhtml'})
                mhtml_content = mhtml_data.get('data', '')
                
                # è¿”å›ç©ºHTMLå’ŒMHTMLæ•°æ®
                return "", mhtml_content.encode('utf-8')
            else:
                # å¤„ç†ç›¸å¯¹é“¾æ¥ï¼Œè½¬æ¢ä¸ºç»å¯¹é“¾æ¥
                soup = BeautifulSoup(html_content, 'lxml')
                
                # è½¬æ¢æ‰€æœ‰ç›¸å¯¹é“¾æ¥
                for tag in soup.find_all(['a', 'link']):
                    if tag.get('href'):
                        href = tag['href']
                        if href.startswith('/') and not href.startswith('//'):
                            tag['href'] = self.base_url + href
                
                for tag in soup.find_all(['img', 'script']):
                    if tag.get('src'):
                        src = tag['src']
                        if src.startswith('/') and not src.startswith('//'):
                            tag['src'] = self.base_url + src
                
                return str(soup), b""
            
        except Exception as e:
            print(f"  Error loading page {url}: {e}")
            error_html = f"<html><body><h1>Error loading page</h1><p>{str(e)}</p></body></html>"
            return error_html, b""
        finally:
            await page.close()
    
    async def download_image(self, img_url: str) -> str:
        """ä¸‹è½½å›¾ç‰‡å¹¶è¿”å›æœ¬åœ°è·¯å¾„"""
        try:
            # è§£ç HTMLå®ä½“ï¼ˆå¦‚ &amp; -> &ï¼‰
            img_url = html.unescape(img_url)
            
            # å¦‚æœå·²ç»ä¸‹è½½è¿‡ï¼Œç›´æ¥è¿”å›
            if img_url in self.downloaded_images:
                return self.downloaded_images[img_url]
            
            # ä¸‹è½½å›¾ç‰‡ï¼ˆå…è®¸é‡å®šå‘ï¼‰
            async with self.session.get(img_url, ssl=False, allow_redirects=True) as response:
                if response.status == 200:
                    content = await response.read()
                    
                    # å°è¯•ä»å¤šä¸ªæ¥æºè·å–æ–‡ä»¶å
                    filename = None
                    
                    # 1. ä»Content-Dispositionå¤´è·å–æ–‡ä»¶å
                    content_disposition = response.headers.get('Content-Disposition', '')
                    if content_disposition:
                        import re
                        match = re.search(r'filename[^;=\n]*=(([\'"]).*?\2|[^;\n]*)', content_disposition)
                        if match:
                            filename = match.group(1).strip('"\'')
                    
                    # 2. ä»æœ€ç»ˆURLï¼ˆé‡å®šå‘åï¼‰è·å–æ–‡ä»¶å
                    if not filename:
                        final_url = str(response.url)
                        # å¦‚æœå‘ç”Ÿäº†é‡å®šå‘ï¼Œæ˜¾ç¤ºä¿¡æ¯
                        if final_url != img_url:
                            print(f"      Redirected to: {final_url}")
                        parsed_final_url = urlparse(final_url)
                        filename = os.path.basename(parsed_final_url.path)
                    
                    # 3. ä»åŸå§‹URLè·å–æ–‡ä»¶åï¼ˆå¦‚æœæ²¡æœ‰é‡å®šå‘ï¼‰
                    if not filename or filename in ['index.php', 'download.php', 'file.php']:
                        parsed_url = urlparse(img_url)
                        filename = os.path.basename(parsed_url.path)
                    
                    # 4. å¦‚æœè¿˜æ˜¯æ²¡æœ‰åˆé€‚çš„æ–‡ä»¶åï¼Œä½¿ç”¨é»˜è®¤åç§°
                    if not filename or filename in ['index.php', 'download.php', 'file.php', '']:
                        # å°è¯•ä»Content-Typeè·å–æ‰©å±•å
                        content_type = response.headers.get('Content-Type', '')
                        ext = '.png'  # é»˜è®¤æ‰©å±•å
                        if 'jpeg' in content_type or 'jpg' in content_type:
                            ext = '.jpg'
                        elif 'png' in content_type:
                            ext = '.png'
                        elif 'gif' in content_type:
                            ext = '.gif'
                        elif 'webp' in content_type:
                            ext = '.webp'
                        elif 'svg' in content_type:
                            ext = '.svg'
                        
                        filename = f"image{ext}"
                    
                    # ç¡®ä¿æ–‡ä»¶åæœ‰æ‰©å±•å
                    if '.' not in filename:
                        # ä»Content-Typeè·å–æ‰©å±•å
                        content_type = response.headers.get('Content-Type', '')
                        if 'jpeg' in content_type or 'jpg' in content_type:
                            filename += '.jpg'
                        elif 'png' in content_type:
                            filename += '.png'
                        elif 'gif' in content_type:
                            filename += '.gif'
                        elif 'webp' in content_type:
                            filename += '.webp'
                        else:
                            filename += '.png'  # é»˜è®¤
                    
                    # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼ˆä½¿ç”¨URLå“ˆå¸Œä½œä¸ºå‰ç¼€ï¼‰
                    url_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]
                    local_filename = f"{url_hash}_{filename}"
                    local_path = self.images_dir / local_filename
                    
                    # ä¿å­˜æ–‡ä»¶
                    async with aiofiles.open(local_path, 'wb') as f:
                        await f.write(content)
                    
                    # è®°å½•æ˜ å°„å…³ç³»
                    self.downloaded_images[img_url] = f"images/{local_filename}"
                    print(f"    Downloaded image: {local_filename}")
                    return f"images/{local_filename}"
                else:
                    print(f"    Failed to download image: {img_url} (Status: {response.status})")
                    return img_url
                    
        except Exception as e:
            print(f"    Error downloading image {img_url}: {e}")
            return img_url
    
    async def convert_to_markdown(self, html_content: str, page_url: str) -> str:
        """å°†HTMLè½¬æ¢ä¸ºMarkdownå¹¶å¤„ç†å›¾ç‰‡"""
        soup = BeautifulSoup(html_content, 'lxml')
        
        # æ¸…ç†ä¸éœ€è¦çš„å¯¼èˆªå…ƒç´ 
        # åˆ é™¤å¸¸è§çš„å¯¼èˆªã€ä¾§è¾¹æ ã€èœå•ç­‰å…ƒç´ 
        selectors_to_remove = [
            # å¯¼èˆªç›¸å…³
            'nav', '.nav', '.navigation', '.navbar', '#nav', '#navigation',
            '.nav-menu', '.menu', '.main-menu', '.side-menu', '.sidebar',
            '.left-menu', '.right-menu', '.top-menu', '.bottom-menu',
            
            # ä¾§è¾¹æ 
            '.sidebar', '.side-bar', '.left-sidebar', '.right-sidebar',
            '#sidebar', 'aside', '.aside',
            
            # é¡µçœ‰é¡µè„š
            'header', '.header', '#header', '.page-header', '.site-header',
            'footer', '.footer', '#footer', '.page-footer', '.site-footer',
            
            # é¢åŒ…å±‘å¯¼èˆª
            '.breadcrumb', '.breadcrumbs', 'nav[aria-label="breadcrumb"]',
            
            # ç›®å½•/ç´¢å¼•
            '.toc', '.table-of-contents', '#toc', '.catalog', '.catalogue',
            
            # å…¶ä»–å¸¸è§çš„éå†…å®¹å…ƒç´ 
            '.ads', '.advertisement', '.banner', '.popup',
            '.modal', '.dialog', '.tooltip',
            
            # ç‰¹å®šäºè¯¥ç½‘ç«™çš„é€‰æ‹©å™¨ï¼ˆæ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´ï¼‰
            '.left-bar', '.right-bar', '.doc-left-nav', '.doc-menu',
            '[class*="menu-"]', '[class*="nav-"]', '[id*="menu-"]', '[id*="nav-"]'
        ]
        
        for selector in selectors_to_remove:
            for element in soup.select(selector):
                element.decompose()
        
        # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸï¼ˆé€šå¸¸åŒ…å«æ–‡ç« ä¸»ä½“ï¼‰
        main_content = None
        content_selectors = [
            'main', 'article', '.content', '.main-content', '#content',
            '.page-content', '.article-content', '.doc-content', '.markdown-body',
            '[role="main"]', '.container > div', '.wrapper > div'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                # é€‰æ‹©åŒ…å«æœ€å¤šæ–‡æœ¬çš„å…ƒç´ ä½œä¸ºä¸»å†…å®¹
                main_content = max(elements, key=lambda x: len(x.get_text(strip=True)))
                break
        
        # å¦‚æœæ‰¾åˆ°ä¸»å†…å®¹åŒºåŸŸï¼Œåªä¿ç•™è¯¥åŒºåŸŸ
        if main_content:
            soup = BeautifulSoup(str(main_content), 'lxml')
        
        # ä¸‹è½½å¹¶æ›¿æ¢æ‰€æœ‰å›¾ç‰‡é“¾æ¥
        for img in soup.find_all('img'):
            if img.get('src'):
                img_url = img['src']
                # è½¬æ¢ä¸ºç»å¯¹URL
                if not img_url.startswith(('http://', 'https://')):
                    img_url = urljoin(page_url, img_url)
                
                print(f"    Processing image: {img_url}")
                
                # ä¸‹è½½å›¾ç‰‡
                local_path = await self.download_image(img_url)
                img['src'] = local_path
        
        # è½¬æ¢ä¸ºMarkdown
        markdown_content = md(str(soup), heading_style="ATX", bullets="-")
        
        # æ¸…ç†è½¬æ¢åçš„Markdown
        # 1. è¯†åˆ«å¹¶åˆ é™¤å¯¼èˆªèœå•å†…å®¹
        lines = markdown_content.split('\n')
        
        # æŸ¥æ‰¾å¯¼èˆªåŒºå—çš„æ¨¡å¼
        # é€šå¸¸å¯¼èˆªèœå•æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š
        # - ä»¥"é¦–é¡µ"é“¾æ¥å¼€å§‹
        # - åŒ…å«ä¸€ç³»åˆ—çŸ­é“¾æ¥æˆ–çŸ­æ–‡æœ¬
        # - åœ¨çœŸæ­£çš„é¡µé¢å†…å®¹ä¹‹å‰
        
        # å¯»æ‰¾å¯¼èˆªåŒºå—
        nav_start = -1
        nav_end = -1
        found_nav_start = False
        consecutive_short_lines = 0
        
        for i, line in enumerate(lines):
            line_text = line.strip()
            
            # è·³è¿‡é¡µé¢æ ‡é¢˜å’Œå…ƒæ•°æ®éƒ¨åˆ†ï¼ˆé€šå¸¸åœ¨å‰10è¡Œå†…ï¼‰
            if i < 10 and ('---' in line_text or '**é¡µé¢ID**' in line_text or 
                          '**ä½œè€…ID**' in line_text or '**åˆ›å»ºæ—¶é—´**' in line_text or 
                          '**åˆ†ç±»ID**' in line_text or line_text.startswith('#')):
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«"é¦–é¡µ"ï¼ˆå¯¼èˆªçš„å¼€å§‹æ ‡å¿—ï¼‰
            if ('é¦–é¡µ' in line_text or '[é¦–é¡µ]' in line_text):
                found_nav_start = True
                if nav_start == -1:
                    nav_start = i
                nav_end = i
            
            # å¦‚æœæ‰¾åˆ°äº†å¯¼èˆªå¼€å§‹
            if found_nav_start:
                # ç©ºè¡Œä¸è®¡å…¥
                if not line_text:
                    # å¦‚æœé‡åˆ°ç©ºè¡Œåé¢è·Ÿç€é•¿å†…å®¹ï¼Œå¯èƒ½æ˜¯æ­£æ–‡å¼€å§‹
                    if i + 1 < len(lines):
                        next_line = lines[i + 1].strip()
                        if len(next_line) > 50:  # é•¿å†…å®¹é€šå¸¸æ˜¯æ­£æ–‡
                            nav_end = i
                            break
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯çŸ­è¡Œï¼ˆå¯¼èˆªé¡¹é€šå¸¸å¾ˆçŸ­ï¼‰
                # å»é™¤é“¾æ¥æ ‡è®°åçš„çº¯æ–‡æœ¬
                pure_text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', line_text)
                
                if len(pure_text) < 30:  # å¯¼èˆªé¡¹é€šå¸¸å°‘äº30ä¸ªå­—ç¬¦
                    consecutive_short_lines += 1
                    nav_end = i + 1
                else:
                    # é‡åˆ°é•¿å†…å®¹ï¼Œå¯èƒ½æ˜¯æ­£æ–‡å¼€å§‹
                    if consecutive_short_lines > 3:  # å¦‚æœå·²ç»æœ‰è¿ç»­çš„çŸ­è¡Œï¼Œè®¤ä¸ºæ‰¾åˆ°äº†å¯¼èˆªåŒºå—
                        nav_end = i
                        break
                    else:
                        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„çŸ­è¡Œï¼Œå¯èƒ½ä¸æ˜¯å¯¼èˆªåŒºå—
                        if consecutive_short_lines < 3:
                            found_nav_start = False
                            nav_start = -1
                            nav_end = -1
                            consecutive_short_lines = 0
        
        # å¦‚æœæ‰¾åˆ°äº†å¯¼èˆªåŒºå—ï¼Œåˆ é™¤å®ƒ
        if found_nav_start and nav_start >= 0 and nav_end > nav_start:
            # ä¿ç•™å¯¼èˆªå‰çš„å†…å®¹å’Œå¯¼èˆªåçš„å†…å®¹
            lines = lines[:nav_start] + lines[nav_end:]
        
        # 2. é¢å¤–æ¸…ç†ï¼šåˆ é™¤é¡µé¢ä¸­é—´å¯èƒ½å‡ºç°çš„å¯¼èˆªèœå•
        # å®šä¹‰å¯¼èˆªå…³é”®è¯
        nav_keywords = {
            'é¦–é¡µ', 'ã€Šç»¿é‚®Â® Xç³»åˆ—å¼€å‘æ¿ã€‹ä½¿ç”¨æŒ‡å—', 'ç®€ä»‹', 'ç‰ˆæœ¬å˜æ›´',
            'ç¡¬ä»¶å®‰è£…', 'å¿«é€Ÿå…¥é—¨', 'æ¥å…¥ç¬¬ä¸‰æ–¹å¹³å°', 'è®¾å¤‡ç®¡ç†åå°',
            'å¼€å‘å®è·µæŒ‡å—', 'æ§åˆ¶å‘½ä»¤', 'æ¨é€æ¶ˆæ¯', 'çŸ­ä¿¡æŒ‡ä»¤', 'å¸¸è§é—®é¢˜'
        }
        
        filtered_lines = []
        skip_count = 0
        
        for i, line in enumerate(lines):
            line_text = line.strip()
            
            # å¦‚æœéœ€è¦è·³è¿‡è¡Œ
            if skip_count > 0:
                skip_count -= 1
                continue
            
            # æå–çº¯æ–‡æœ¬ï¼ˆå»é™¤Markdownè¯­æ³•ï¼‰
            pure_text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', line_text)
            pure_text = re.sub(r'[#*`_~]', '', pure_text).strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¼èˆªèœå•çš„å¼€å§‹
            if pure_text in nav_keywords:
                # å‘å‰æŸ¥çœ‹ï¼Œè®¡ç®—è¿ç»­çš„å¯¼èˆªé¡¹æ•°é‡
                nav_count = 1
                for j in range(i + 1, min(i + 20, len(lines))):
                    next_line = lines[j].strip()
                    next_pure = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', next_line)
                    next_pure = re.sub(r'[#*`_~]', '', next_pure).strip()
                    
                    if next_pure in nav_keywords or len(next_pure) < 20:
                        nav_count += 1
                    else:
                        break
                
                # å¦‚æœè¿ç»­å¯¼èˆªé¡¹è¶…è¿‡3ä¸ªï¼Œè·³è¿‡æ•´ä¸ªåŒºå—
                if nav_count >= 3:
                    skip_count = nav_count - 1
                    continue
            
            # ä¿ç•™éå¯¼èˆªå†…å®¹
            filtered_lines.append(line)
        
        markdown_content = '\n'.join(filtered_lines)
        
        # 3. æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
        
        # 4. åˆ é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½
        markdown_content = markdown_content.strip()
        
        return markdown_content

    async def save_page_content(self, page_info: Dict[str, Any], content: tuple[str, bytes], file_path: Path, page_url: str = None):
        """ä¿å­˜é¡µé¢å†…å®¹ï¼ˆHTMLã€MHTMLæˆ–Markdownï¼‰"""
        try:
            if self.save_format == "mhtml" and content[1]:
                # ä¿å­˜MHTMLæ–‡ä»¶
                with open(file_path, 'wb') as f:
                    f.write(content[1])
                print(f"  Saved MHTML: {file_path}")
            elif self.save_format == "markdown":
                markdown_content = content[0]
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä»APIè·å–çš„åŸå§‹Markdown
                if 'page_content' in page_info and content[0] == page_info.get('page_content'):
                    # è¿™æ˜¯ä»APIè·å–çš„åŸå§‹Markdownï¼Œéœ€è¦å¤„ç†
                    # 1. è§£ç HTMLå®ä½“ï¼ˆå¦‚ &quot; -> ", &amp; -> &, ç­‰ï¼‰
                    markdown_content = html.unescape(markdown_content)
                    
                    # 2. å¤„ç†å›¾ç‰‡é“¾æ¥
                    # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡é“¾æ¥å¹¶ä¸‹è½½
                    import re
                    img_pattern = r'!\[([^\]]*)\]\(([^\)]+)\)'
                    
                    async def replace_image(match):
                        alt_text = match.group(1)
                        img_url = match.group(2)
                        
                        # å¤„ç†ç›¸å¯¹URL
                        if not img_url.startswith(('http://', 'https://')):
                            img_url = urljoin(self.base_url, img_url)
                        
                        print(f"    Processing image: {img_url}")
                        local_path = await self.download_image(img_url)
                        return f'![{alt_text}]({local_path})'
                    
                    # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡é“¾æ¥
                    for match in re.finditer(img_pattern, markdown_content):
                        img_url = match.group(2)
                        alt_text = match.group(1)
                        
                        # å¤„ç†ç›¸å¯¹URL
                        if not img_url.startswith(('http://', 'https://')):
                            img_url = urljoin(self.base_url, img_url)
                        
                        print(f"    Processing image: {img_url}")
                        local_path = await self.download_image(img_url)
                        markdown_content = markdown_content.replace(match.group(0), f'![{alt_text}]({local_path})')
                    
                    # 3. æ¸…ç†HTMLæ ‡ç­¾ï¼ˆå¦‚ <center> ç­‰ï¼‰
                    markdown_content = re.sub(r'</?center\s*>', '', markdown_content)
                    # æ³¨æ„ï¼šHTMLå®ä½“å·²ç»è¢«è§£ç ï¼Œæ‰€ä»¥ä¸éœ€è¦å¤„ç† &lt; å’Œ &gt;
                    
                    # 4. æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                    markdown_content = markdown_content.strip()
                    
                else:
                    # è¿™æ˜¯ä»HTMLè½¬æ¢æ¥çš„Markdownï¼Œå·²ç»å¤„ç†è¿‡äº†
                    markdown_content = await self.convert_to_markdown(content[0], page_url)
                
                # æ·»åŠ é¡µé¢å…ƒæ•°æ®åˆ°Markdownå¼€å¤´
                page_title = page_info.get('page_title', 'Untitled')
                metadata = f"""# {page_title}

---
- **é¡µé¢ID**: {page_info.get('page_id', '')}
- **ä½œè€…ID**: {page_info.get('author_uid', '')}
- **åˆ›å»ºæ—¶é—´**: {page_info.get('addtime', '') or page_info.get('page_addtime', '')}
- **åˆ†ç±»ID**: {page_info.get('cat_id', '')}
---

"""
                
                # ä¿å­˜Markdownæ–‡ä»¶
                async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                    await f.write(metadata + markdown_content)
                
                print(f"  Saved Markdown: {file_path}")
            else:
                # ä¿å­˜HTMLæ–‡ä»¶
                html_content = content[0]
                
                # åœ¨HTMLä¸­åµŒå…¥é¡µé¢å…ƒæ•°æ®
                soup = BeautifulSoup(html_content, 'lxml')
                
                # æ·»åŠ å…ƒæ•°æ®åˆ°head
                if not soup.head:
                    soup.html.insert(0, soup.new_tag('head'))
                
                # æ·»åŠ é¡µé¢ä¿¡æ¯ä½œä¸ºmetaæ ‡ç­¾
                meta_info = {
                    'page-id': page_info.get('page_id', ''),
                    'page-title': page_info.get('page_title', ''),
                    'author-uid': page_info.get('author_uid', ''),
                    'add-time': page_info.get('addtime', ''),
                    'cat-id': page_info.get('cat_id', '')
                }
                
                for name, content_value in meta_info.items():
                    meta_tag = soup.new_tag('meta', attrs={'name': name, 'content': str(content_value)})
                    soup.head.append(meta_tag)
                
                # ä¿å­˜HTMLæ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                
                print(f"  Saved HTML: {file_path}")
            
        except Exception as e:
            print(f"  Error saving {file_path}: {e}")

    async def process_page(self, page_info: Dict[str, Any], parent_path: Path, item_id: str = None):
        """å¤„ç†å•ä¸ªé¡µé¢"""
        page_id = page_info.get('page_id')
        
        # é¿å…é‡å¤å¤„ç†
        if page_id in self.processed_pages:
            return
        self.processed_pages.add(page_id)
        
        page_title = page_info.get('page_title', 'Untitled')
        page_title_clean = self.sanitize_filename(page_title)
        
        # ä½¿ç”¨ä¼ å…¥çš„item_idæˆ–é»˜è®¤çš„self.item_id
        current_item_id = item_id or self.item_id
        
        print(f"Processing page: {page_title} (Item: {current_item_id}, Page: {page_id})")
        
        # æ ¹æ®æ ¼å¼ç¡®å®šæ–‡ä»¶æ‰©å±•åå’Œè·å–æ–¹å¼
        if self.save_format == "mhtml":
            file_ext = ".mhtml"
            # è·å–é¡µé¢å†…å®¹ï¼ˆé€šè¿‡æµè§ˆå™¨ï¼‰
            content = await self.get_page_content(current_item_id, page_id)
        elif self.save_format == "markdown":
            file_ext = ".md"
            # ç›´æ¥é€šè¿‡APIè·å–Markdownå†…å®¹
            markdown_data = self.get_page_markdown(page_id)
            if markdown_data:
                # ä½¿ç”¨APIè¿”å›çš„æ›´å®Œæ•´ä¿¡æ¯æ›´æ–°page_info
                page_info.update(markdown_data)
                content = (markdown_data.get('page_content', ''), b"")
            else:
                # å¦‚æœAPIå¤±è´¥ï¼Œè·³è¿‡è¯¥é¡µé¢æˆ–å°è¯•åˆå§‹åŒ–æµè§ˆå™¨
                print(f"  Failed to get markdown via API")
                if not self.browser:
                    print(f"  Skipping page due to no browser initialized in Markdown mode")
                    return
                else:
                    print(f"  Falling back to browser method")
                    content = await self.get_page_content(current_item_id, page_id)
        else:
            file_ext = ".html"
            # è·å–é¡µé¢å†…å®¹ï¼ˆé€šè¿‡æµè§ˆå™¨ï¼‰
            content = await self.get_page_content(current_item_id, page_id)
            
        file_name = f"{page_id}_{page_title_clean}{file_ext}"
        file_path = parent_path / file_name
        
        # æ„å»ºé¡µé¢URL
        page_url = f"{self.web_url}/{current_item_id}/{page_id}"
        
        await self.save_page_content(page_info, content, file_path, page_url)
        
        # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(0.5)  # å‡å°‘å»¶è¿Ÿï¼Œå› ä¸ºAPIè°ƒç”¨æ›´å¿«

    async def process_catalog(self, catalog: Dict[str, Any], parent_path: Path, item_id: str = None):
        """é€’å½’å¤„ç†ç›®å½•å’Œé¡µé¢"""
        # è·å–ç›®å½•åç§°å¹¶æ¸…ç†
        cat_name = catalog.get('cat_name', 'Unnamed')
        cat_name_clean = self.sanitize_filename(cat_name)
        cat_path = parent_path / cat_name_clean
        
        # ä½¿ç”¨catalogä¸­çš„item_idæˆ–ä¼ å…¥çš„item_idæˆ–é»˜è®¤å€¼
        current_item_id = catalog.get('item_id') or item_id or self.item_id
        
        print(f"\nProcessing catalog: {cat_name} (Item: {current_item_id})")
        cat_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç›®å½•ç´¢å¼•æ–‡ä»¶
        index_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{cat_name}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        ul {{ list-style-type: none; padding: 0; }}
        li {{ margin: 10px 0; }}
        a {{ text-decoration: none; color: #0066cc; }}
        a:hover {{ text-decoration: underline; }}
    </style>
</head>
<body>
    <h1>{cat_name}</h1>
    <ul>
"""
        
        # å¤„ç†è¯¥ç›®å½•ä¸‹çš„é¡µé¢
        pages = catalog.get('pages', [])
        for page in pages:
            await self.process_page(page, cat_path, current_item_id)
            # æ·»åŠ åˆ°ç´¢å¼•
            page_title = page.get('page_title', 'Untitled')
            page_id = page.get('page_id')
            if self.save_format == "mhtml":
                file_ext = ".mhtml"
            elif self.save_format == "markdown":
                file_ext = ".md"
            else:
                file_ext = ".html"
            file_name = f"{page_id}_{self.sanitize_filename(page_title)}{file_ext}"
            index_content += f'        <li><a href="{file_name}">{page_title}</a></li>\n'
        
        # é€’å½’å¤„ç†å­ç›®å½•
        sub_catalogs = catalog.get('catalogs', [])
        if sub_catalogs:
            index_content += '        <li><hr></li>\n'
            index_content += '        <li><strong>å­ç›®å½•:</strong></li>\n'
            
        for sub_catalog in sub_catalogs:
            sub_cat_name = sub_catalog.get('cat_name', 'Unnamed')
            sub_cat_clean = self.sanitize_filename(sub_cat_name)
            index_content += f'        <li>ğŸ“ <a href="{sub_cat_clean}/index.html">{sub_cat_name}</a></li>\n'
            await self.process_catalog(sub_catalog, cat_path, current_item_id)
        
        index_content += """    </ul>
</body>
</html>"""
        
        # ä¿å­˜ç›®å½•ç´¢å¼•
        with open(cat_path / "index.html", 'w', encoding='utf-8') as f:
            f.write(index_content)

    async def scrape_site(self, headless=True, save_format="mhtml"):
        """çˆ¬å–æ•´ä¸ªç½‘ç«™"""
        self.save_format = save_format
        print(f"Starting to scrape site with item_id: {self.item_id}")
        print(f"Save format: {self.save_format}")
        
        # å¦‚æœæ˜¯Markdownæ ¼å¼ï¼Œå¯ä»¥é€‰æ‹©ä¸ä½¿ç”¨æµè§ˆå™¨
        if self.save_format == "markdown":
            print("Using API mode for Markdown format (faster)")
            # ä»ç„¶éœ€è¦åˆå§‹åŒ–sessionç”¨äºä¸‹è½½å›¾ç‰‡
            self.session = aiohttp.ClientSession(
                headers=self.headers,
                cookies=self.cookies
            )
        else:
            print(f"Browser mode: {'Headless' if headless else 'Visible'}")
            # åˆå§‹åŒ–æµè§ˆå™¨
            await self.init_browser(headless=headless)
        
        try:
            # è·å–ç½‘ç«™ç»“æ„
            site_data = self.get_api_data("539")  # ä½¿ç”¨é»˜è®¤é¡µé¢ID
            
            if not site_data:
                print("Failed to get site structure")
                return
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_dir.mkdir(parents=True, exist_ok=True)
            
            # å¦‚æœæ˜¯Markdownæ ¼å¼ï¼Œåˆ›å»ºimagesç›®å½•
            if self.save_format == "markdown":
                self.images_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç½‘ç«™å…ƒæ•°æ®
            with open(self.output_dir / 'site_structure.json', 'w', encoding='utf-8') as f:
                json.dump(site_data, f, ensure_ascii=False, indent=2)
            
            menu = site_data.get('menu', {})
            
            # åˆ›å»ºä¸»ç´¢å¼•æ–‡ä»¶
            item_name = site_data.get('item_name', 'Documentation')
            index_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{item_name}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        ul {{ list-style-type: none; padding: 0; }}
        li {{ margin: 10px 0; }}
        a {{ text-decoration: none; color: #0066cc; }}
        a:hover {{ text-decoration: underline; }}
        .section {{ margin-top: 30px; }}
    </style>
</head>
<body>
    <h1>{item_name}</h1>
    
    <div class="section">
        <h2>é¡µé¢åˆ—è¡¨</h2>
        <ul>
"""
            
            # å¤„ç†æ ¹ç›®å½•ä¸‹çš„é¡µé¢
            root_pages = menu.get('pages', [])
            current_item_id = site_data.get('item_id', self.item_id)
            
            for page in root_pages:
                await self.process_page(page, self.output_dir, current_item_id)
                # æ·»åŠ åˆ°ä¸»ç´¢å¼•
                page_title = page.get('page_title', 'Untitled')
                page_id = page.get('page_id')
                if self.save_format == "mhtml":
                    file_ext = ".mhtml"
                elif self.save_format == "markdown":
                    file_ext = ".md"
                else:
                    file_ext = ".html"
                file_name = f"{page_id}_{self.sanitize_filename(page_title)}{file_ext}"
                index_content += f'            <li><a href="{file_name}">{page_title}</a></li>\n'
            
            # å¤„ç†ç›®å½•ç»“æ„
            catalogs = menu.get('catalogs', [])
            if catalogs:
                index_content += """        </ul>
    </div>
    
    <div class="section">
        <h2>ç›®å½•ç»“æ„</h2>
        <ul>
"""
            
            for catalog in catalogs:
                cat_name = catalog.get('cat_name', 'Unnamed')
                cat_clean = self.sanitize_filename(cat_name)
                index_content += f'            <li>ğŸ“ <a href="{cat_clean}/index.html">{cat_name}</a></li>\n'
                await self.process_catalog(catalog, self.output_dir, current_item_id)
            
            index_content += """        </ul>
    </div>
</body>
</html>"""
            
            # ä¿å­˜ä¸»ç´¢å¼•æ–‡ä»¶
            with open(self.output_dir / "index.html", 'w', encoding='utf-8') as f:
                f.write(index_content)
            
            print(f"\nScraping completed!")
            print(f"All content saved to: {self.output_dir}")
            print(f"Total pages processed: {len(self.processed_pages)}")
            
        finally:
            # å…³é—­èµ„æº
            if self.save_format == "markdown":
                # å…³é—­aiohttp session
                if self.session:
                    await self.session.close()
            else:
                # å…³é—­æµè§ˆå™¨
                await self.close_browser()


async def main():
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    headless = True
    save_format = "mhtml"  # é»˜è®¤ä½¿ç”¨MHTMLæ ¼å¼
    
    if '--show-browser' in sys.argv:
        headless = False
        print("Running with visible browser for debugging...")
    
    if '--html' in sys.argv:
        save_format = "html"
        print("Using HTML format (without embedded resources)")
    
    if '--markdown' in sys.argv or '--md' in sys.argv:
        save_format = "markdown"
        print("Using Markdown format (with local images)")
    
    scraper = LvyaTechScraper()
    await scraper.scrape_site(headless=headless, save_format=save_format)


if __name__ == "__main__":
    asyncio.run(main())