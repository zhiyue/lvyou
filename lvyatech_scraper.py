#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import json
import os
import re
import time
from pathlib import Path
from typing import Any, Dict, List
from urllib.parse import unquote

import requests
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup


class LvyaTechScraper:
    def __init__(self):
        self.base_url = "http://www.lvyatech.com:37788"
        self.api_url = f"{self.base_url}/server/index.php?s=/api/item/info"
        self.web_url = f"{self.base_url}/web/#"
        self.headers = {
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'en,zh;q=0.9,zh-CN;q=0.8,zh-HK;q=0.7,zh-TW;q=0.6',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Origin': self.base_url,
            'Pragma': 'no-cache',
            'Referer': f'{self.base_url}/web/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
        }
        self.cookies = {
            'cookie_name': 'value',
            'think_language': 'en',
            'PHPSESSID': '7o9vc6er8cjrdioi2reea1ob37'
        }
        self.item_id = "39"
        self.output_dir = Path("lvyatech_html_docs")
        self.browser = None
        self.context = None
        self.processed_pages = set()
        self.save_format = "mhtml"  # å¯é€‰: "html" æˆ– "mhtml"

    def sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•çš„å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        filename = ''.join(char for char in filename if ord(char) >= 32)
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        return filename.strip()

    def get_api_data(self, page_id: str) -> Dict[str, Any]:
        """é€šè¿‡APIè·å–é¡µé¢æ•°æ®"""
        data = {
            'item_id': self.item_id,
            'keyword': '',
            'default_page_id': page_id
        }
        
        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                cookies=self.cookies,
                data=data,
                verify=False
            )
            response.raise_for_status()
            result = response.json()
            
            if result.get('error_code') == 0:
                return result.get('data', {})
            else:
                print(f"Error getting page {page_id}: {result}")
                return {}
                
        except Exception as e:
            print(f"Exception getting API data for page {page_id}: {e}")
            return {}

    async def init_browser(self, headless=True):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=self.headers['User-Agent']
        )
        
        # è®¾ç½®cookies
        cookies = []
        for name, value in self.cookies.items():
            cookies.append({
                'name': name,
                'value': value,
                'domain': 'www.lvyatech.com',
                'path': '/'
            })
        await self.context.add_cookies(cookies)

    async def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()

    async def get_page_content(self, item_id: str, page_id: str) -> tuple[str, bytes]:
        """è·å–åŠ¨æ€æ¸²æŸ“åçš„é¡µé¢å†…å®¹ï¼ˆHTMLæˆ–MHTMLï¼‰"""
        # æ„å»ºæ­£ç¡®çš„URLæ ¼å¼: http://www.lvyatech.com:37788/web/#/{item_id}/{page_id}
        url = f"{self.web_url}/{item_id}/{page_id}"
        print(f"  Loading page: {url}")
        
        page = await self.context.new_page()
        try:
            # å¯¼èˆªåˆ°é¡µé¢
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            
            # å¤šé‡ç­‰å¾…ç­–ç•¥ç¡®ä¿é¡µé¢å®Œå…¨æ¸²æŸ“
            # 1. ç­‰å¾…ç½‘ç»œç©ºé—²
            await page.wait_for_load_state('networkidle')
            
            # 2. ç­‰å¾…å¯èƒ½çš„å†…å®¹å®¹å™¨
            content_selectors = [
                '.page-content',
                '.content-wrapper', 
                '.main-content',
                '[class*="content"]',
                '.markdown-body',
                '#content',
                'article',
                'main',
                '.container',
                'pre',  # ä»£ç å—
                'table' # è¡¨æ ¼
            ]
            
            # å°è¯•ç­‰å¾…ä»»æ„ä¸€ä¸ªå†…å®¹é€‰æ‹©å™¨
            try:
                await page.wait_for_selector(
                    ', '.join(content_selectors), 
                    state='visible',
                    timeout=10000
                )
            except:
                pass
            
            # 3. ç­‰å¾…ç‰¹å®šæ—¶é—´ç¡®ä¿JavaScriptæ‰§è¡Œå®Œæˆ
            await page.wait_for_timeout(3000)
            
            # 4. ç­‰å¾…æ‰€æœ‰å›¾ç‰‡åŠ è½½
            await page.evaluate("""
                () => Promise.all(
                    Array.from(document.images)
                        .filter(img => !img.complete)
                        .map(img => new Promise(resolve => {
                            img.onload = img.onerror = resolve;
                        }))
                )
            """)
            
            # 5. æ£€æŸ¥æ˜¯å¦æœ‰åŠ¨æ€å†…å®¹åŠ è½½æŒ‡ç¤ºå™¨
            loading_selectors = [
                '.loading',
                '.spinner',
                '[class*="load"]',
                '.skeleton'
            ]
            
            # ç­‰å¾…åŠ è½½æŒ‡ç¤ºå™¨æ¶ˆå¤±
            for selector in loading_selectors:
                try:
                    await page.wait_for_selector(selector, state='hidden', timeout=5000)
                except:
                    pass
            
            # 6. æœ€åå†ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ‰€æœ‰å¼‚æ­¥æ“ä½œå®Œæˆ
            await page.wait_for_timeout(2000)
            
            # 7. æ™ºèƒ½ç­‰å¾…ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å®é™…å†…å®¹
            max_wait_time = 15000  # æœ€å¤šç­‰å¾…15ç§’
            start_time = asyncio.get_event_loop().time()
            
            while (asyncio.get_event_loop().time() - start_time) * 1000 < max_wait_time:
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å®è´¨å†…å®¹
                content_check = await page.evaluate("""
                    () => {
                        // è·å–é¡µé¢æ–‡æœ¬å†…å®¹é•¿åº¦
                        const bodyText = document.body ? document.body.innerText.trim() : '';
                        const hasContent = bodyText.length > 100;
                        
                        // æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç å—æˆ–è¡¨æ ¼ç­‰ç‰¹æ®Šå†…å®¹
                        const hasCodeBlocks = document.querySelectorAll('pre, code').length > 0;
                        const hasTables = document.querySelectorAll('table').length > 0;
                        const hasImages = document.querySelectorAll('img').length > 0;
                        
                        // æ£€æŸ¥æ˜¯å¦è¿˜åœ¨åŠ è½½ä¸­
                        const loadingElements = document.querySelectorAll('.loading, .spinner, [class*="load"]');
                        const isLoading = Array.from(loadingElements).some(el => 
                            el.offsetParent !== null && window.getComputedStyle(el).display !== 'none'
                        );
                        
                        return {
                            hasContent: hasContent || hasCodeBlocks || hasTables || hasImages,
                            contentLength: bodyText.length,
                            isLoading: isLoading
                        };
                    }
                """)
                
                if content_check['hasContent'] and not content_check['isLoading']:
                    print(f"    Content loaded (length: {content_check['contentLength']} chars)")
                    break
                
                await page.wait_for_timeout(500)
            
            # æ ¹æ®æ ¼å¼è·å–å†…å®¹
            if self.save_format == "mhtml":
                # ä½¿ç”¨CDPè·å–MHTML
                client = await page.context.new_cdp_session(page)
                mhtml_data = await client.send('Page.captureSnapshot', {'format': 'mhtml'})
                mhtml_content = mhtml_data.get('data', '')
                
                # è¿”å›ç©ºHTMLå’ŒMHTMLæ•°æ®
                return "", mhtml_content.encode('utf-8')
            else:
                # è·å–æ¸²æŸ“åçš„HTML
                html_content = await page.content()
                
                # å¤„ç†ç›¸å¯¹é“¾æ¥ï¼Œè½¬æ¢ä¸ºç»å¯¹é“¾æ¥
                soup = BeautifulSoup(html_content, 'lxml')
                
                # è½¬æ¢æ‰€æœ‰ç›¸å¯¹é“¾æ¥
                for tag in soup.find_all(['a', 'link']):
                    if tag.get('href'):
                        href = tag['href']
                        if href.startswith('/') and not href.startswith('//'):
                            tag['href'] = self.base_url + href
                
                for tag in soup.find_all(['img', 'script']):
                    if tag.get('src'):
                        src = tag['src']
                        if src.startswith('/') and not src.startswith('//'):
                            tag['src'] = self.base_url + src
                
                return str(soup), b""
            
        except Exception as e:
            print(f"  Error loading page {url}: {e}")
            error_html = f"<html><body><h1>Error loading page</h1><p>{str(e)}</p></body></html>"
            return error_html, b""
        finally:
            await page.close()

    async def save_page_content(self, page_info: Dict[str, Any], content: tuple[str, bytes], file_path: Path):
        """ä¿å­˜é¡µé¢å†…å®¹ï¼ˆHTMLæˆ–MHTMLï¼‰"""
        try:
            if self.save_format == "mhtml" and content[1]:
                # ä¿å­˜MHTMLæ–‡ä»¶
                with open(file_path, 'wb') as f:
                    f.write(content[1])
                print(f"  Saved MHTML: {file_path}")
            else:
                # ä¿å­˜HTMLæ–‡ä»¶
                html_content = content[0]
                
                # åœ¨HTMLä¸­åµŒå…¥é¡µé¢å…ƒæ•°æ®
                soup = BeautifulSoup(html_content, 'lxml')
                
                # æ·»åŠ å…ƒæ•°æ®åˆ°head
                if not soup.head:
                    soup.html.insert(0, soup.new_tag('head'))
                
                # æ·»åŠ é¡µé¢ä¿¡æ¯ä½œä¸ºmetaæ ‡ç­¾
                meta_info = {
                    'page-id': page_info.get('page_id', ''),
                    'page-title': page_info.get('page_title', ''),
                    'author-uid': page_info.get('author_uid', ''),
                    'add-time': page_info.get('addtime', ''),
                    'cat-id': page_info.get('cat_id', '')
                }
                
                for name, content_value in meta_info.items():
                    meta_tag = soup.new_tag('meta', attrs={'name': name, 'content': str(content_value)})
                    soup.head.append(meta_tag)
                
                # ä¿å­˜HTMLæ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                
                print(f"  Saved HTML: {file_path}")
            
        except Exception as e:
            print(f"  Error saving {file_path}: {e}")

    async def process_page(self, page_info: Dict[str, Any], parent_path: Path, item_id: str = None):
        """å¤„ç†å•ä¸ªé¡µé¢"""
        page_id = page_info.get('page_id')
        
        # é¿å…é‡å¤å¤„ç†
        if page_id in self.processed_pages:
            return
        self.processed_pages.add(page_id)
        
        page_title = page_info.get('page_title', 'Untitled')
        page_title_clean = self.sanitize_filename(page_title)
        
        # ä½¿ç”¨ä¼ å…¥çš„item_idæˆ–é»˜è®¤çš„self.item_id
        current_item_id = item_id or self.item_id
        
        print(f"Processing page: {page_title} (Item: {current_item_id}, Page: {page_id})")
        
        # è·å–é¡µé¢å†…å®¹
        content = await self.get_page_content(current_item_id, page_id)
        
        # æ ¹æ®æ ¼å¼ç¡®å®šæ–‡ä»¶æ‰©å±•å
        file_ext = ".mhtml" if self.save_format == "mhtml" else ".html"
        file_name = f"{page_id}_{page_title_clean}{file_ext}"
        file_path = parent_path / file_name
        
        await self.save_page_content(page_info, content, file_path)
        
        # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(1)

    async def process_catalog(self, catalog: Dict[str, Any], parent_path: Path, item_id: str = None):
        """é€’å½’å¤„ç†ç›®å½•å’Œé¡µé¢"""
        # è·å–ç›®å½•åç§°å¹¶æ¸…ç†
        cat_name = catalog.get('cat_name', 'Unnamed')
        cat_name_clean = self.sanitize_filename(cat_name)
        cat_path = parent_path / cat_name_clean
        
        # ä½¿ç”¨catalogä¸­çš„item_idæˆ–ä¼ å…¥çš„item_idæˆ–é»˜è®¤å€¼
        current_item_id = catalog.get('item_id') or item_id or self.item_id
        
        print(f"\nProcessing catalog: {cat_name} (Item: {current_item_id})")
        cat_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç›®å½•ç´¢å¼•æ–‡ä»¶
        index_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{cat_name}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        ul {{ list-style-type: none; padding: 0; }}
        li {{ margin: 10px 0; }}
        a {{ text-decoration: none; color: #0066cc; }}
        a:hover {{ text-decoration: underline; }}
    </style>
</head>
<body>
    <h1>{cat_name}</h1>
    <ul>
"""
        
        # å¤„ç†è¯¥ç›®å½•ä¸‹çš„é¡µé¢
        pages = catalog.get('pages', [])
        for page in pages:
            await self.process_page(page, cat_path, current_item_id)
            # æ·»åŠ åˆ°ç´¢å¼•
            page_title = page.get('page_title', 'Untitled')
            page_id = page.get('page_id')
            file_ext = ".mhtml" if self.save_format == "mhtml" else ".html"
            file_name = f"{page_id}_{self.sanitize_filename(page_title)}{file_ext}"
            index_content += f'        <li><a href="{file_name}">{page_title}</a></li>\n'
        
        # é€’å½’å¤„ç†å­ç›®å½•
        sub_catalogs = catalog.get('catalogs', [])
        if sub_catalogs:
            index_content += '        <li><hr></li>\n'
            index_content += '        <li><strong>å­ç›®å½•:</strong></li>\n'
            
        for sub_catalog in sub_catalogs:
            sub_cat_name = sub_catalog.get('cat_name', 'Unnamed')
            sub_cat_clean = self.sanitize_filename(sub_cat_name)
            index_content += f'        <li>ğŸ“ <a href="{sub_cat_clean}/index.html">{sub_cat_name}</a></li>\n'
            await self.process_catalog(sub_catalog, cat_path, current_item_id)
        
        index_content += """    </ul>
</body>
</html>"""
        
        # ä¿å­˜ç›®å½•ç´¢å¼•
        with open(cat_path / "index.html", 'w', encoding='utf-8') as f:
            f.write(index_content)

    async def scrape_site(self, headless=True, save_format="mhtml"):
        """çˆ¬å–æ•´ä¸ªç½‘ç«™"""
        self.save_format = save_format
        print(f"Starting to scrape site with item_id: {self.item_id}")
        print(f"Browser mode: {'Headless' if headless else 'Visible'}")
        print(f"Save format: {self.save_format}")
        
        # åˆå§‹åŒ–æµè§ˆå™¨
        await self.init_browser(headless=headless)
        
        try:
            # è·å–ç½‘ç«™ç»“æ„
            site_data = self.get_api_data("539")  # ä½¿ç”¨é»˜è®¤é¡µé¢ID
            
            if not site_data:
                print("Failed to get site structure")
                return
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç½‘ç«™å…ƒæ•°æ®
            with open(self.output_dir / 'site_structure.json', 'w', encoding='utf-8') as f:
                json.dump(site_data, f, ensure_ascii=False, indent=2)
            
            menu = site_data.get('menu', {})
            
            # åˆ›å»ºä¸»ç´¢å¼•æ–‡ä»¶
            item_name = site_data.get('item_name', 'Documentation')
            index_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{item_name}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        ul {{ list-style-type: none; padding: 0; }}
        li {{ margin: 10px 0; }}
        a {{ text-decoration: none; color: #0066cc; }}
        a:hover {{ text-decoration: underline; }}
        .section {{ margin-top: 30px; }}
    </style>
</head>
<body>
    <h1>{item_name}</h1>
    
    <div class="section">
        <h2>é¡µé¢åˆ—è¡¨</h2>
        <ul>
"""
            
            # å¤„ç†æ ¹ç›®å½•ä¸‹çš„é¡µé¢
            root_pages = menu.get('pages', [])
            current_item_id = site_data.get('item_id', self.item_id)
            
            for page in root_pages:
                await self.process_page(page, self.output_dir, current_item_id)
                # æ·»åŠ åˆ°ä¸»ç´¢å¼•
                page_title = page.get('page_title', 'Untitled')
                page_id = page.get('page_id')
                file_ext = ".mhtml" if self.save_format == "mhtml" else ".html"
                file_name = f"{page_id}_{self.sanitize_filename(page_title)}{file_ext}"
                index_content += f'            <li><a href="{file_name}">{page_title}</a></li>\n'
            
            # å¤„ç†ç›®å½•ç»“æ„
            catalogs = menu.get('catalogs', [])
            if catalogs:
                index_content += """        </ul>
    </div>
    
    <div class="section">
        <h2>ç›®å½•ç»“æ„</h2>
        <ul>
"""
            
            for catalog in catalogs:
                cat_name = catalog.get('cat_name', 'Unnamed')
                cat_clean = self.sanitize_filename(cat_name)
                index_content += f'            <li>ğŸ“ <a href="{cat_clean}/index.html">{cat_name}</a></li>\n'
                await self.process_catalog(catalog, self.output_dir, current_item_id)
            
            index_content += """        </ul>
    </div>
</body>
</html>"""
            
            # ä¿å­˜ä¸»ç´¢å¼•æ–‡ä»¶
            with open(self.output_dir / "index.html", 'w', encoding='utf-8') as f:
                f.write(index_content)
            
            print(f"\nScraping completed!")
            print(f"All content saved to: {self.output_dir}")
            print(f"Total pages processed: {len(self.processed_pages)}")
            
        finally:
            # å…³é—­æµè§ˆå™¨
            await self.close_browser()


async def main():
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    headless = True
    save_format = "mhtml"  # é»˜è®¤ä½¿ç”¨MHTMLæ ¼å¼
    
    if '--show-browser' in sys.argv:
        headless = False
        print("Running with visible browser for debugging...")
    
    if '--html' in sys.argv:
        save_format = "html"
        print("Using HTML format (without embedded resources)")
    
    scraper = LvyaTechScraper()
    await scraper.scrape_site(headless=headless, save_format=save_format)


if __name__ == "__main__":
    asyncio.run(main())